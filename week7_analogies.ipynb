{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NollyKeyz/NLP/blob/main/week7_analogies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load pre-trained GloVe model\n",
        "\n",
        "The GloVe model is a method to convert words into numbers. It looks at how often words appear together in text and assigns each word a unique number so that similar words have similar numbers. This helps computers understand the meaning of words and find relationships between them, like \"king\" being similar to \"queen.\"\n",
        "\n",
        "Gensim is an open-source Python library designed for topic modeling, document indexing, and similarity retrieval with large corpora. It is widely used for natural language processing (NLP) and is particularly known for its implementations of word embedding models like Word2Vec, Doc2Vec, and FastText. Gensim provides simple and efficient tools for vector space modeling and allows users to easily train and utilize word embeddings for various NLP tasks."
      ],
      "metadata": {
        "id": "Pxy3gR8NyXD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P2FR2BXdxagL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2acb50-252d-4e23-c5d4-a0baca0af600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find analogy using word embeddings"
      ],
      "metadata": {
        "id": "clFjMIqA0guD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find analogy using word embeddings\n",
        "def find_analogy(word1, word2, word3, topn=3):\n",
        "    try:\n",
        "        # Perform vector arithmetic to find the analogy\n",
        "        analogy = model.most_similar(positive=[word1, word2], negative=[word3], topn=topn)\n",
        "        return analogy\n",
        "    except KeyError as e:\n",
        "        print(f\"Word not found in vocabulary: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "D_SHBCIi0hEK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example analogy"
      ],
      "metadata": {
        "id": "JwO34guuyeYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = \"king\"\n",
        "word2 = \"man\"\n",
        "word3 = \"woman\"\n",
        "# Find the analogy\n",
        "analogy = find_analogy(word1, word2, word3)\n",
        "analogy"
      ],
      "metadata": {
        "id": "_T4NjVoVyesc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0b602a-4814-4bf0-ba81-1070f95279ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('prince', 0.6826401948928833),\n",
              " ('brother', 0.6500723958015442),\n",
              " ('ii', 0.6345422267913818)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Better Model\n",
        "The word2vec-google-news-300 is a pre-trained word embedding model trained on a large corpus of Google News articles. It was trained using the word2vec algorithm and consists of word vectors of dimensionality 300. This model captures semantic relationships between words based on the context in which they appear in the training data. These word embeddings are useful for various natural language processing tasks such as word similarity, analogy detection, and text classification."
      ],
      "metadata": {
        "id": "z3fi1-Yl3R7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "\n",
        "# Download and load pre-trained word embeddings model\n",
        "model = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "aD2m-2uc3SFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28cb9ee-6ece-4ff9-98ff-6c9acd99f37c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define analogy relationship"
      ],
      "metadata": {
        "id": "xDfK8uBx8AMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analogy = (input('enter the origin word: '), input('enter the word relating to the origin: '), input('enter the word relating to the expected: '))\n",
        "\n",
        "# Calculate analogy\n",
        "try:\n",
        "    result = model.most_similar(positive=[analogy[0], analogy[2]], negative=[analogy[1]], topn=1)\n",
        "    print(f\"Analogous word to '{analogy[0]}' 'in relation to ' '{analogy[1]}' 'and ' '{analogy[2]}' is: '{result[0][0]}'\")\n",
        "except KeyError:\n",
        "    print(\"One or more words not found in vocabulary.\")"
      ],
      "metadata": {
        "id": "Sol8o93K8AVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99618e65-0223-4a58-9a3a-aafe22ebd95c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the origin word: tokyo\n",
            "enter the word relating to the origin: japan\n",
            "enter the word relating to the expected: germany\n",
            "Analogous word to 'tokyo' 'in relation to ' 'japan' 'and ' 'germany' is: 'sweden'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please try the below:\n",
        "\n",
        "\"Paris\" - \"France\" + \"Italy\" ≈ \"\"\n",
        "\n",
        "\"Tokyo\" - \"Japan\" + \"Germany\" ≈ \"\"\n",
        "\n",
        "\"Apple\" - \"Fruit\" + \"Electronics\" ≈ \"\"\n",
        "\n",
        "\"Facebook\" - \"Social media\" + \"Search\" ≈ \"\"\n",
        "\n",
        "\"Cat\" - \"Pet\" + \"Bird\" ≈ \"\""
      ],
      "metadata": {
        "id": "jAqfnwKP9hDW"
      }
    }
  ]
}